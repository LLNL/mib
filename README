****************************************************************************\
  $Id: README,v 1.9 2005/11/30 20:24:57 auselton Exp $
****************************************************************************
  Copyright (C) 2001-2002 The Regents of the University of California.
  Produced at Lawrence Livermore National Laboratory (cf, DISCLAIMER).
  Written by Andrew Uselton <uselton2@llnl.gov>
  UCRL-CODE-2006-xxx.
  
  This file is part of Mib, an MPI-based parallel I/O benchamrk
  For details, see <http://www.llnl.gov/linux/mib/>.
  
  Mib is free software; you can redistribute it and/or modify it under
  the terms of the GNU General Public License as published by the Free
  Software Foundation; either version 2 of the License, or (at your option)
  any later version.
  
  Mib is distributed in the hope that it will be useful, but WITHOUT 
  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or 
  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License 
  for more details.
  
  You should have received a copy of the GNU General Public License along
  with Mib; if not, write to the Free Software Foundation, Inc.,
  59 Temple Place, Suite 330, Boston, MA  02111-1307  USA.
****************************************************************************/

			       MIB
		    MPI (POSIX) I/O BENCHMARK
			Andrew C. Uselton
		     Updated March 20, 2006

Mib is an outgrowth of Lustre (http://www.lustre.org/) benchmarking
efforts at LLNL, though its operation in no way requires Lustre.  It
resembles the IOR benchmark (ftp://ftp.llnl.gov/pub/siop/ior/) in that
it carries out I/O to a parallel file system from within a coordinated
parallel MPI application.  It is otherwise entirely distinct and
all-new code.

Mib has two advantages over IOR.  First, it is very simple.  Following
the idea of "do only one thing, and do it well," mib avoids the
complexity and size associated with IOR.  It loads quickly.  It is
easily modified to suit specific local needs.  Second, it will
optionally produce a "profile" of the timing of every system call.
These timings are synchronized and coordinated accross all tasks.
Post-processing tools (see the "tools" subdirectory) can manipulate
the profiles to give a graphical view of the dynamic behavior of the
file system under the load provided by mib.  These graphs have proven
valuable in isolating performance bottlenecks in Lustre.  

Mib is characterized by the following:
o Both MPI and non-MPI operation
o POSIX system calls only
o One file per MPI task
o The test runs exactly once with the given parameters

usage: mib [ab::d:EIhHl:L:MnpPrRs:St:W]
    -a              :  Use average profile times accross node.
    -b [<gran>]     :  Random seeks (optional granularity) before each read
                    :  seek(fd, gran*file_size*rand()/RAND_MAX, SEEK_SET
    -d <log_dir>    :  parameters file "<log_dir>/options"
                    :    and profiles files here, if any 
                    :    (default is cwd).
    -E              :  Show environment of test in output.
    -I              :  Show intermediate values in output.
    -h              :  This message
    -H              :  Show headers in output.
    -l <call_limit> :  Issue no more than this many system calls.
    -L <time_limit> :  Do not issue new system calls after this many
                    :    seconds (limits are per phase for write and
                    :    read phases).
    -M              :  Do not use MPI even if it is available.
    -n              :  Create new files if files were already present
                    :    (will always create new files if none were
                    :    present).
    -p              :  Output system call timing profiles to <log_dir>
                    :    (default is current working directory).
    -P              :  Show progress bars during testing.
    -r              :  Remove files when done.
    -R              :  Only perform the read test.
    -s <call_size>  :  Use system calls of this size (default 512k).
                    :    Numbers may use abreviations k, K, m, or M.
    -S              :  Show signon message, including date, program
                    :    name, and version number.
    -t <test_dir>   :  I/O transactions to and from this directory
                    :    (default is current working directory).
    -W              :  Only perform the write test
                    :    (if -R and -W are both present both tests 
                    :     will run, but that's the default anyway)

In the LLNL Linux clusters environment the SLURM
(http://www.llnl.gov/linux/slurm/) command "srun" launches MPI
applications.  A typical invokation might be 

srun -N8 -p ltest mib -t /p/ga2/lustre-test/mib/newfiles
 |    |      |     |       |
 |    |      |     |       ---target directory
 |    |      |     -----------MPI application
 |    |      -----------------partition (group of target nodes)
 |    ------------------------number of tasks
 -----------------------------SLURM MPI launcher

This example would run 8 MPI tasks on nodes taken from the "ltest"
group of compute nodes and the mib executable would open, write to,
close, open, read from, and close, a unique file for each task all in
the given directory.  The I/O would continue for the default amount of
time or the default number of transfers (whichever comes frst), and
would involve system calls using the default amount of data.  In the
following discussion assume the following shell variables are defined
to create the above command line: 

SCOM="srun -N8 -p ltest"
MCOM="mib -t /p/ga2/lustre-test/mib/newfiles"

The most common use would include a limit on the number of system
calls, a limit on time, and a system call size.  For instance if we
wanted to perform per taks no more than 4096 system calls of 512k
bytes each and to cease issuing system calls after 60 seconds, the
command would look like this:

$SCOM $MCOM -l 1024 -L 60 -s 512k

A typical result for the above command (as observed on the ALC
cluster) would look like this:


Fri Mar 17 22:50:59 2006      8    4M  4500   300	897.07     566.20

The "-H" option provides an informative header:

$SCOM $MCOM -l 1024 -L 60 -s 512k -H

           date            tasks  xfer  call  time      write       read
                                       limit limit       MB/s       MB/s
 ------------------------ ------ ----- ----- ----- ---------- ----------
 Fri Mar 17 22:50:59 2006      8    4M  4500   300     897.07      566.20

The next most common use for mib is to generate a system casll timing
profile.  The command:

$SCOM $MCOM -l 1024 -L 60 -s 512k -d ~/alc/ -p

creates two files, "~/alc/write.profile" and "~/alc/read.profile".
Each of those is a table with one line per system call, and on each
line, one floating point number per task.  Each number is the
timestamp of the corresponding system call (i.e. the MPI_Wtime() at
which it returned) on the corresponding task.  The timestamps are all
relative to a "zero" at the beginning of the test coordinated accross
all tasksm and adjusted for any skew in the "MPI_Wtime()" global clock
reading.  


